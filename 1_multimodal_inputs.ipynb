{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52955d3e",
   "metadata": {},
   "source": [
    "# 1. Working with Multimodal Inputs\n",
    "\n",
    "In this notebook, we'll explore how to work with multimodal inputs (text, images, and videos) using Amazon Bedrock's Nova model. We'll build on the prompt engineering concepts from the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621886a",
   "metadata": {},
   "source": [
    "## Setting up Bedrock client for multimodal inputs\n",
    "\n",
    "Let's set up the Bedrock client using our utility functions that support multimodal inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c8d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('./')\n",
    "\n",
    "from src.utils import (\n",
    "    create_bedrock_client,\n",
    "    invoke_with_image,\n",
    "    invoke_with_video,\n",
    "    extract_json_from_text,\n",
    "    invoke_with_prefill,\n",
    "    NOVA_LITE\n",
    ")\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock_client = create_bedrock_client()\n",
    "\n",
    "# Default settings\n",
    "TEMPERATURE = 0.0  # Lower temperature = more deterministic outputs\n",
    "\n",
    "# Set up assets directory - updated to use root assets folder\n",
    "current_dir = Path.cwd()\n",
    "assets_dir = current_dir.joinpath('assets')\n",
    "print('The assets are located in:', assets_dir)\n",
    "\n",
    "# Verify the assets directory exists\n",
    "if not assets_dir.exists():\n",
    "    print(\"WARNING: Assets directory not found. Please update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde2bd0",
   "metadata": {},
   "source": [
    "## 1. Basic Image Understanding\n",
    "\n",
    "Let's start with a simple prompt to describe an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f'{assets_dir}/image.png'\n",
    "\n",
    "basic_prompt = \"Describe what's happening in the image above.\"\n",
    "\n",
    "response = invoke_with_image(bedrock_client, image_path, basic_prompt, model_id=NOVA_LITE, temperature=TEMPERATURE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869f036",
   "metadata": {},
   "source": [
    "## 2. Structured Image Analysis\n",
    "\n",
    "Let's add structure to our image analysis prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_image_prompt = \"\"\"\n",
    "## Instructions\n",
    "You are an image analysis tool. \n",
    "Given the image above, provide a clear and comprehensive description following the rules below.\n",
    "\n",
    "## Rules\n",
    "- Describe the scene in detail, including people, objects, and actions\n",
    "- Comment on the setting and environment\n",
    "- Note any interesting or unusual elements\n",
    "- Describe the image in an engaging way, as if you are a sports commentator\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_with_image(bedrock_client, image_path, structured_image_prompt, model_id=NOVA_LITE, temperature=TEMPERATURE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fad02",
   "metadata": {},
   "source": [
    "## 3. Video Understanding\n",
    "\n",
    "Now let's analyze a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ef6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = f'{assets_dir}/video.mp4'\n",
    "\n",
    "video_prompt = \"\"\"\n",
    "## Instructions\n",
    "You are a video analysis tool. \n",
    "Given the video above, provide a clear and comprehensive description of the video following the rules below.\n",
    "\n",
    "## Rules\n",
    "- Describe the key scenes in the video in chronological order\n",
    "- Note any changes or movements that occur throughout the video\n",
    "- Comment on people, objects, actions, and setting\n",
    "- Describe the video in an engaging way\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_with_video(bedrock_client, video_path, video_prompt, model_id=NOVA_LITE, temperature=TEMPERATURE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f330fa5",
   "metadata": {},
   "source": [
    "## 4. Structured JSON Output from Images with Prefilling\n",
    "\n",
    "Let's extract structured information from an image and return it as JSON, using prefilling to guarantee proper formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f07a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our structured analysis schema as a JSON object\n",
    "image_schema = {\n",
    "    \"scene_type\": \"string (indoor/outdoor)\",\n",
    "    \"time_of_day\": \"string (day/night/unclear)\",\n",
    "    \"num_people\": \"number\",\n",
    "    \"activities\": [\"string\"],\n",
    "    \"objects\": [\"string\"],\n",
    "    \"mood\": \"string (happy/sad/neutral/excited/etc.)\",\n",
    "    \"description\": \"string (1-2 sentence summary)\"\n",
    "}\n",
    "\n",
    "# Inject the schema directly into the prompt\n",
    "json_image_prompt = f\"\"\"\n",
    "## Instructions\n",
    "You are an image analysis tool. \n",
    "Given the image above, analyze it carefully and extract the requested information in the schema provided below.\n",
    "\n",
    "## Schema\n",
    "{image_schema}\n",
    "\n",
    "## Rules\n",
    "- Analyze the image carefully and provide all requested information\n",
    "- Return a valid and parseable JSON object inside ```json code blocks\n",
    "- Do not include any explanation or text outside the JSON object\n",
    "\"\"\"\n",
    "\n",
    "# Using prefilling to ensure we get a properly formatted JSON response\n",
    "prefill = \"\"\"\n",
    "```json\n",
    "{\n",
    "  \"scene_type\": \"\"\"\n",
    "\n",
    "# Invoke model with prefill to ensure proper JSON structure\n",
    "result = invoke_with_prefill(bedrock_client, \n",
    "                            prompt=json_image_prompt, \n",
    "                            prefill=prefill, \n",
    "                            image_path=image_path,\n",
    "                            model_id=NOVA_LITE, \n",
    "                            temperature=TEMPERATURE)\n",
    "\n",
    "# Combine the prefill with the completion for the full response\n",
    "full_result = prefill + result\n",
    "print(full_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49330e1",
   "metadata": {},
   "source": [
    "## 5. Extracting and Using the JSON Output\n",
    "\n",
    "Let's extract the JSON from the prefilled response and use it programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    image_data = extract_json_from_text(full_result)\n",
    "    \n",
    "    # Access specific fields\n",
    "    print(f\"Scene type: {image_data['scene_type']}\")\n",
    "    print(f\"Time of day: {image_data['time_of_day']}\")\n",
    "    print(f\"Number of people: {image_data['num_people']}\")\n",
    "    \n",
    "    print(\"\\nActivities:\")\n",
    "    for activity in image_data[\"activities\"]:\n",
    "        print(f\"- {activity}\")\n",
    "        \n",
    "    print(\"\\nObjects:\")\n",
    "    for obj in image_data[\"objects\"]:\n",
    "        print(f\"- {obj}\")\n",
    "        \n",
    "    print(f\"\\nMood: {image_data['mood']}\")\n",
    "    print(f\"\\nDescription: {image_data['description']}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error extracting JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479826e",
   "metadata": {},
   "source": [
    "## 6. Structured Video Analysis with Prefilling\n",
    "\n",
    "Now let's apply the same approach to video analysis, using prefilling to ensure we get properly formatted JSON output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our video analysis schema as a structured data object\n",
    "video_analysis_schema = {\n",
    "    \"duration_impression\": \"string (short/medium/long)\",\n",
    "    \"num_people\": \"number\",\n",
    "    \"num_animals\": \"number\", \n",
    "    \"scenes\": [\"string (descriptions of key moments)\"],\n",
    "    \"action_summary\": \"string (1-2 sentence summary of activity)\",\n",
    "    \"location\": \"string (where the video takes place)\"\n",
    "}\n",
    "\n",
    "# Using an f-string to inject our schema directly into the prompt\n",
    "prefill_video_prompt = f\"\"\"\n",
    "## Instructions\n",
    "You are a video analysis tool. \n",
    "Given the video above, analyze it carefully and extract the requested information according to the schema.\n",
    "\n",
    "## Schema\n",
    "{video_analysis_schema}\n",
    "\n",
    "## Rules\n",
    "- Analyze the video carefully and provide all requested information\n",
    "- Return a valid and parseable JSON object inside ```json code blocks\n",
    "- Use chain-of-thought reasoning by first describing what you see, then filling in the schema\n",
    "\"\"\"\n",
    "\n",
    "# Start the response with the JSON structure\n",
    "prefill = \"\"\"\n",
    "```json\n",
    "{\n",
    "  \"duration_impression\": \"\"\"\n",
    "\n",
    "# Invoke model with prefill to ensure proper JSON structure\n",
    "video_result = invoke_with_prefill(bedrock_client, \n",
    "                                  prompt=prefill_video_prompt, \n",
    "                                  prefill=prefill, \n",
    "                                  video_path=video_path,\n",
    "                                  model_id=NOVA_LITE, \n",
    "                                  temperature=TEMPERATURE)\n",
    "\n",
    "# Combine prefill with completion for the full response\n",
    "full_video_result = prefill + video_result\n",
    "print(full_video_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa374ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and use the structured data from video analysis\n",
    "try:\n",
    "    video_data = extract_json_from_text(full_video_result)\n",
    "    \n",
    "    print(f\"Video duration impression: {video_data['duration_impression']}\")\n",
    "    print(f\"Number of people: {video_data['num_people']}\")\n",
    "    print(f\"Number of animals: {video_data['num_animals']}\")\n",
    "    print(f\"Location: {video_data['location']}\")\n",
    "    \n",
    "    print(\"\\nKey scenes:\")\n",
    "    for i, scene in enumerate(video_data[\"scenes\"], 1):\n",
    "        print(f\"{i}. {scene}\")\n",
    "        \n",
    "    print(f\"\\nAction summary: {video_data['action_summary']}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error extracting JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897685d",
   "metadata": {},
   "source": [
    "## 7. Chain of Thought Reasoning with Images\n",
    "\n",
    "Chain of thought reasoning helps the model work through problems step by step, which often improves accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our questions as a structured data object\n",
    "questions = [\n",
    "    \"What is the weather condition in this image?\",\n",
    "    \"What activities are people engaged in?\",\n",
    "    \"What time of day does this appear to be?\",\n",
    "    \"Are there any safety concerns visible?\",\n",
    "    \"What type of environment is shown?\"\n",
    "]\n",
    "\n",
    "# Build prompt with chain-of-thought reasoning\n",
    "specific_questions_prompt = f\"\"\"\n",
    "## Instructions\n",
    "Look at the image above and answer the following questions:\n",
    "\n",
    "{chr(10).join(f\"{i+1}. {question}\" for i, question in enumerate(questions))}\n",
    "\n",
    "## Rules\n",
    "- First, describe what you see in the image in detail\n",
    "- Then, think through each question step by step\n",
    "- Finally, provide numbered answers to match each question (1-2 sentences per answer)\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_with_image(bedrock_client, image_path, specific_questions_prompt, model_id=NOVA_LITE, temperature=TEMPERATURE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Asking Specific Questions About Images\n",
    "\n",
    "We can also use the model to answer specific questions about an image, which is useful for extracting targeted information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_question = \"How many people are in this image, and what are they wearing?\"\n",
    "\n",
    "response = invoke_with_image(bedrock_client, image_path, specific_question, model_id=NOVA_LITE, temperature=TEMPERATURE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building Real-world Applications\n",
    "\n",
    "With the skills you've learned in this notebook, you can build various real-world multimodal applications such as:\n",
    "\n",
    "1. **Visual content moderation**: Analyzing images for inappropriate content\n",
    "2. **Visual search**: Finding products based on image inputs\n",
    "3. **Content cataloging**: Automatically tagging and organizing media\n",
    "4. **Accessibility tools**: Creating descriptions of images for visually impaired users\n",
    "5. **Security applications**: Analyzing surveillance footage\n",
    "6. **Educational tools**: Creating interactive learning experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "In this notebook, we've explored multimodal inputs with Amazon Bedrock's Nova model:\n",
    "\n",
    "1. Basic image understanding\n",
    "2. Structured image analysis \n",
    "3. Video understanding\n",
    "4. Structured JSON outputs with prefilling for guaranteed format\n",
    "5. Extracting and using structured data from media analysis\n",
    "6. Chain-of-thought reasoning with visual inputs\n",
    "7. Asking specific questions about images and videos\n",
    "\n",
    "Next, explore the various design patterns in the `patterns/` directory to learn about implementing GenAI patterns like prompt chaining, routing, and orchestration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaihackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
